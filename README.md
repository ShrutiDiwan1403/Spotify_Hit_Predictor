# Spotify_Hit_Predictor
**Objective Summary**: Labouring with the Spotify dataset is to predict a hit song using different soundtrack feature.The other sub-objective is to improve the research of musicologists employing exploratory and predictive analysis. Simultaneously enhance the creativity of the music virtuoso. 


**Description and Insight**: Music is a vocal or instrumental sound or combination of both fused in a way that produces a beautiful form of harmony and expression of emotion. Music is the language of the universe. It brings people together. It not only inspires people but also improves focus and memory and improves cognitive abilities. In addition, it sometimes lowers the blood pressure if listened correctly. In other words, music is not the solution to all the problems, but it can help tackle the issue better. The purpose of labouring with the Spotify dataset is to predict a hit song using different soundtrack features. Also, along the way, find the relationship between other elements in the suggested music dataset from 1960 to 2019. The database comprises American and Latin American artists only. The other sub-objective is to improve the research of musicologists employing exploratory and predictive analysis. Simultaneously enhance the creativity of the music virtuoso. The future goal of the study is to discover the hit predictor for each artist from 1960 to 2019 based on their song 
information available in the existing dataset.


The final selected dataset has a meticulous range of fields to give more information about the track (song), but the database did not consist of field genre from a domain point of view. Therefore, found a similar dataset consisting of genre fields and linked it to the original dataset in the tableau.

**Programming Language**:  --


**Tool**: Tableau


**Glimpse of Dashboards**:

*Energy Dashboard*

![Energy_Dashboard](https://github.com/user-attachments/assets/09698702-a481-4ab4-90e3-d2079b80242d)


*Target Audience and decision-making process*: The prime beneficiary of this layout will be the **artists themselves, music composers, and listeners**. First, the performer/singer will recognize the key and genre they are more comfortable with while creating the melody. What is their liveness when they sing a song with a particular key? Finally, they can ascertain their energy range. And work on improving it if required. Listeners can pick songs and artists depending upon the energy range they favour. They can also consider the genre. The layout will work as a blueprint for the music composers can work on developing music after understanding the energy range of the artist. They 
can also unveil the energy trend in different decades before releasing their work. Since energy is **the sense of forwarding motion in music, whatever keeps the listener engaged and listening**. Composers can **write and arrange music for various media, including film, tv, stage productions, video games, and advertisements, as per the energy requirement in that scenario**. 


*Purpose*: The intent here is to find the liveness of the melody for all the musical keys. The other aspiration is to find the relationship of energy feature with other remaining features of the songs. 


*Information it presents*: The dashboard exhibits the maximum energy for the different genres for each decade. While the track's energy is found based on the two attributes. At the same time, the summation of energy of all the ways for all the artists can be noted, which is segregated by decade. Both leading and featuring artists are exerted to unveil the total energy here. Suppose only the leading artist attribute is considered. In that case, the energy value of the main artist who also appeared as a featuring artist will get factored out from the closing value. And affect the overall result. Finally, the liveness of tracks by 12 musical keys for each decade can also be seen. Maximum energy is used instead of minimum energy in the first chart. When the graph was formed using the min value, the range was assigned to infer the musical piece with minimum energy. It was noticed that many artists have a high energy range while recording a track. And so, the maximum function was added to set out better results. 


*Actionable Insights*: When the dashboard is filtered by the 'Energy of Artist' graph that is any jitter (each jitter represents a distinct artist from that era), then the maximum energy of the artist for all the genres they have sung the song in is displayed. The liveness of all the keys for different decades for all the tracks sang by a particular artist is exhibited. 



*Valence Dashboard*

![Valence_Dashboard](https://github.com/user-attachments/assets/23a9fe95-71e7-40c1-a776-6afbc7ce4a2c)


**Note: The valence attribute had myriad values between the range of 0.0 and 1.0. For better graphical presentation and analysis. There was categorizing a numerical value to cheerful, happy, and sad depressing**. 

*Target Audience and decision-making process*: The target customers to utilize the dashboard are listeners, music composers, audio engineers, and songwriters. The layout will help the listeners select a song based on their mood. They don't need to apprehend what instrumental value is; they can opt for the theme just established founded by decade and genre. The information portrayed on the dashboard will suggest songwriters, audio engineers, and music composers select the instrumental value while making the song keeping the genre and the valence in mind. The engineers, composers, and writers must work collectively to settle the song's mood. Otherwise, unexpected, 
funny music pieces will be created. For instance, a rap song with high instrumental value and not much rap content cannot be classified as a rap song as it will not meet the standard. 


*Purpose*: The layout intends to light the number of cheerful, happy, and sad depressing songs each decade. At the same time, also track down the melody based on genre. And determine the valence by genre. Eventually, the purpose is to find the average instrumental value for each valence type. 

*Information it presents and Actionable Insights*: It was realized that the maximum number of tracks were sung in 1960 in the POP genre. And most of the songs were happy songs in all six decades. It is generally believed that sad and depressing songs are more instrumental than other types of music. The above exploratory analysis confirmed the belief that the average instrumental value of the sad depressing piece is more than happy or cheerful songs. It is further followed by cheerful and happy music, respectively. Dazing information revealed from the dashboard was that the average instrumental value for the sad and depressing track was in the rock genre. But there was an expectation that the maximum average weight for sad songs would be revealed in Latin or pop music. The instrumental-valence graph corroborated that the rap songs are more speechy and less instrumental. The happy rap song has the most negligible instrumental value of 0.0320. The cheerful music in the rock genre has almost the same value as sad music in the rap category. The value is 
close to 0.2. 

The was a downfall in the number of pop track compositions till late 2000. But a spike was seen from the beginning and throughout the 2010 decade. On the other hand, the number of EDM tracks created was constantly low over six decades.


**Dashboard Link**: https://public.tableau.com/app/profile/shruti.sindhi6021/viz/Spotify_Analysis_16971477136270/


**Performance Outcome**:


![vinyl___](https://github.com/user-attachments/assets/408fd118-fe26-47bd-bf24-5700c4ad6b78)
